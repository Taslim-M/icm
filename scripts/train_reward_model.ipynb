{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/codelion/icm/blob/main/scripts/train_reward_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reward-model-title"
   },
   "source": [
    "# Training a Reward Model with ICM Dataset\n",
    "\n",
    "This notebook demonstrates how to train a reward model using the dataset generated by Internal Coherence Maximization (ICM). The dataset contains instruction-output pairs where outputs are binary (True/False) labels, making it ideal for reward modeling.\n",
    "\n",
    "**Dataset:** `codelion/Qwen3-0.6B-icm`  \n",
    "**Base Model:** `Qwen/Qwen3-0.6B`  \n",
    "**Output Model:** `codelion/Qwen3-0.6B-icm-rm`\n",
    "\n",
    "## Prerequisites\n",
    "- Set your Hugging Face token as `HF_TOKEN` in Colab secrets\n",
    "- GPU runtime recommended for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-section"
   },
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-dependencies"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers>=4.36.0 \\\n",
    "    datasets>=2.14.0 \\\n",
    "    torch>=2.0.0 \\\n",
    "    accelerate>=0.24.0 \\\n",
    "    peft>=0.6.0 \\\n",
    "    trl>=0.7.0 \\\n",
    "    huggingface_hub>=0.19.0 \\\n",
    "    wandb \\\n",
    "    scipy \\\n",
    "    scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from huggingface_hub import login, HfApi\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import wandb\n",
    "from google.colab import userdata\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Login to Hugging Face\n",
    "HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "print(f\"üöÄ Using device: {torch.cuda.get_device_name() if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"üìä Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\" if torch.cuda.is_available() else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data-loading"
   },
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-dataset"
   },
   "outputs": [],
   "source": [
    "# Load the ICM dataset\n",
    "print(\"üìÅ Loading ICM dataset...\")\n",
    "dataset = load_dataset(\"codelion/Qwen3-0.6B-icm\")\n",
    "\n",
    "print(f\"Dataset structure: {dataset}\")\n",
    "print(f\"Number of examples: {len(dataset['train'])}\")\n",
    "\n",
    "# Display sample data\n",
    "sample = dataset['train'][0]\n",
    "print(\"\\nüìù Sample data:\")\n",
    "print(f\"Instruction: {sample['instruction'][:200]}...\")\n",
    "print(f\"Output: {sample['output']}\")\n",
    "\n",
    "# Check label distribution\n",
    "labels = [item['output'] for item in dataset['train']]\n",
    "label_counts = pd.Series(labels).value_counts()\n",
    "print(f\"\\nüìä Label distribution:\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"  {label}: {count} ({count/len(labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "preprocess-data"
   },
   "outputs": [],
   "source": [
    "# Convert labels to binary (0 for False, 1 for True)\n",
    "def preprocess_data(examples):\n",
    "    # Convert True/False strings to binary labels\n",
    "    labels = []\n",
    "    for output in examples['output']:\n",
    "        if output.strip().lower() == 'true':\n",
    "            labels.append(1)\n",
    "        elif output.strip().lower() == 'false':\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            # Handle edge cases - default to 0\n",
    "            print(f\"‚ö†Ô∏è Unexpected output: {output}, defaulting to 0\")\n",
    "            labels.append(0)\n",
    "    \n",
    "    return {\n",
    "        'text': examples['instruction'],\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# Apply preprocessing\n",
    "processed_dataset = dataset.map(\n",
    "    preprocess_data,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Preprocessed dataset: {processed_dataset}\")\n",
    "print(f\"Sample processed item: {processed_dataset['train'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "train-test-split"
   },
   "outputs": [],
   "source": "from datasets import ClassLabel\n\n# Convert labels column to ClassLabel feature for stratification\nprocessed_dataset = processed_dataset.cast_column(\"labels\", ClassLabel(names=[\"False\", \"True\"]))\n\n# Split data into train/validation sets\ntrain_test_split = processed_dataset['train'].train_test_split(\n    test_size=0.2, \n    stratify_by_column='labels',\n    seed=42\n)\n\ntrain_dataset = train_test_split['train']\neval_dataset = train_test_split['test']\n\nprint(f\"üìä Training examples: {len(train_dataset)}\")\nprint(f\"üìä Validation examples: {len(eval_dataset)}\")\n\n# Check label balance in splits\ntrain_labels = pd.Series(train_dataset['labels']).value_counts()\neval_labels = pd.Series(eval_dataset['labels']).value_counts()\n\nprint(\"\\nüìà Training set label distribution:\")\nfor label, count in train_labels.items():\n    print(f\"  {label}: {count} ({count/len(train_dataset)*100:.1f}%)\")\n\nprint(\"\\nüìà Validation set label distribution:\")\nfor label, count in eval_labels.items():\n    print(f\"  {label}: {count} ({count/len(eval_dataset)*100:.1f}%)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model-setup"
   },
   "source": [
    "## Model Setup and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-model-tokenizer"
   },
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
    "OUTPUT_MODEL_NAME = \"codelion/Qwen3-0.6B-icm-rm\"\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"üî§ Loading tokenizer: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"‚úÖ Set pad_token to eos_token\")\n",
    "\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
    "print(f\"Pad token: {tokenizer.pad_token}\")\n",
    "print(f\"EOS token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tokenize-data"
   },
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding=False,  # We'll pad dynamically\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # Add labels\n",
    "    tokenized['labels'] = examples['labels']\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Apply tokenization\n",
    "print(\"üî§ Tokenizing datasets...\")\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "tokenized_eval = eval_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    remove_columns=eval_dataset.column_names\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Tokenized training examples: {len(tokenized_train)}\")\n",
    "print(f\"‚úÖ Tokenized validation examples: {len(tokenized_eval)}\")\n",
    "\n",
    "# Check tokenized sample\n",
    "sample_tokenized = tokenized_train[0]\n",
    "print(f\"\\nüìù Sample tokenized input length: {len(sample_tokenized['input_ids'])}\")\n",
    "print(f\"üìù Sample label: {sample_tokenized['labels']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-reward-model"
   },
   "outputs": [],
   "source": [
    "# Load model for sequence classification (reward modeling)\n",
    "print(f\"ü§ñ Loading model: {MODEL_NAME}\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2,  # Binary classification (False=0, True=1)\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "# Resize token embeddings if needed\n",
    "if len(tokenizer) != model.config.vocab_size:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(f\"‚úÖ Resized token embeddings to {len(tokenizer)}\")\n",
    "\n",
    "# Set pad_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Model dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training-setup"
   },
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training-args"
   },
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen3-reward-model\",\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    \n",
    "    # Evaluation and saving\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=250,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=250,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=50,\n",
    "    logging_dir=\"./logs\",\n",
    "    \n",
    "    # Performance\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=2,\n",
    "    \n",
    "    # Early stopping\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_accuracy\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=42,\n",
    "    \n",
    "    # Disable wandb for now (can be enabled if needed)\n",
    "    report_to=[],\n",
    "    \n",
    "    # Push to hub\n",
    "    push_to_hub=False,  # We'll do this manually\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training arguments configured\")\n",
    "print(f\"üìä Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"üîÑ Total training steps: {len(tokenized_train) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data-collator-metrics"
   },
   "outputs": [],
   "source": [
    "# Data collator for padding\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Define evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Data collator and metrics configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training-section"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-trainer"
   },
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer created\")\n",
    "print(f\"üìä Training dataset size: {len(trainer.train_dataset)}\")\n",
    "print(f\"üìä Evaluation dataset size: {len(trainer.eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start-training"
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(\"This may take 30-60 minutes depending on your GPU.\")\n",
    "\n",
    "# Train the model\n",
    "training_result = trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")\n",
    "print(f\"üìä Final training loss: {training_result.training_loss:.4f}\")\n",
    "print(f\"üïí Training time: {training_result.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"‚ö° Training samples per second: {training_result.metrics['train_samples_per_second']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation-section"
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final-evaluation"
   },
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "print(\"üìä Running final evaluation...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nüìà Final Evaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    if key.startswith('eval_'):\n",
    "        metric_name = key.replace('eval_', '').title()\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {metric_name}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {metric_name}: {value}\")\n",
    "\n",
    "# Test some predictions\n",
    "print(\"\\nüîç Testing predictions on sample data...\")\n",
    "sample_texts = [\n",
    "    \"Question: What is 2+2?\\nClaim: 2+2 = 4\\nI think this claim is\",\n",
    "    \"Question: What is 2+2?\\nClaim: 2+2 = 5\\nI think this claim is\",\n",
    "    \"Question: Is the sky blue?\\nClaim: The sky is blue\\nI think this claim is\"\n",
    "]\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "        confidence = predictions[0][predicted_class].item()\n",
    "    \n",
    "    result = \"True\" if predicted_class == 1 else \"False\"\n",
    "    print(f\"  Sample {i+1}: {result} (confidence: {confidence:.3f})\")\n",
    "    print(f\"    Text: {text[:50]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save-upload-section"
   },
   "source": [
    "## Save and Upload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-model"
   },
   "outputs": [],
   "source": [
    "# Save the model locally\n",
    "print(\"üíæ Saving model locally...\")\n",
    "trainer.save_model(\"./qwen3-reward-model-final\")\n",
    "tokenizer.save_pretrained(\"./qwen3-reward-model-final\")\n",
    "\n",
    "print(\"‚úÖ Model saved locally\")\n",
    "\n",
    "# Create model card\n",
    "model_card = f\"\"\"\n",
    "---\n",
    "language: en\n",
    "tags:\n",
    "- reward-model\n",
    "- icm\n",
    "- qwen\n",
    "- binary-classification\n",
    "datasets:\n",
    "- codelion/Qwen3-0.6B-icm\n",
    "base_model: Qwen/Qwen3-0.6B\n",
    "metrics:\n",
    "- accuracy\n",
    "- f1\n",
    "pipeline_tag: text-classification\n",
    "---\n",
    "\n",
    "# Qwen3-0.6B ICM Reward Model\n",
    "\n",
    "This is a reward model trained on the ICM (Internal Coherence Maximization) dataset using the Qwen3-0.6B base model. The model performs binary classification to determine if a claim is True or False.\n",
    "\n",
    "## Model Details\n",
    "\n",
    "- **Base Model:** Qwen/Qwen3-0.6B\n",
    "- **Training Dataset:** codelion/Qwen3-0.6B-icm\n",
    "- **Task:** Binary classification (True/False)\n",
    "- **Training Framework:** Transformers\n",
    "\n",
    "## Performance\n",
    "\n",
    "- **Accuracy:** {eval_results.get('eval_accuracy', 'N/A'):.4f}\n",
    "- **F1 Score:** {eval_results.get('eval_f1', 'N/A'):.4f}\n",
    "- **Precision:** {eval_results.get('eval_precision', 'N/A'):.4f}\n",
    "- **Recall:** {eval_results.get('eval_recall', 'N/A'):.4f}\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codelion/Qwen3-0.6B-icm-rm\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"codelion/Qwen3-0.6B-icm-rm\")\n",
    "\n",
    "# Example usage\n",
    "text = \"Question: What is 2+2?\\\\nClaim: 2+2 = 4\\\\nI think this claim is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "    \n",
    "result = \"True\" if predicted_class == 1 else \"False\"\n",
    "print(f\"Prediction: {{result}}\")\n",
    "```\n",
    "\n",
    "## Training Details\n",
    "\n",
    "- **Training Framework:** Hugging Face Transformers\n",
    "- **Optimizer:** AdamW\n",
    "- **Learning Rate:** 2e-5\n",
    "- **Batch Size:** 8 (with gradient accumulation)\n",
    "- **Epochs:** 3\n",
    "- **Hardware:** Google Colab GPU\n",
    "\n",
    "## Applications\n",
    "\n",
    "This reward model can be used for:\n",
    "- Reinforcement Learning from Human Feedback (RLHF)\n",
    "- Quality assessment of generated text\n",
    "- Truth/falsehood classification tasks\n",
    "- Fine-tuning other language models\n",
    "\n",
    "## Citation\n",
    "\n",
    "If you use this model, please cite the ICM paper:\n",
    "\n",
    "```bibtex\n",
    "@software{{icm,\n",
    "  title = {{ICM: Internal Coherence Maximization}},\n",
    "  author = {{Asankhaya Sharma}},\n",
    "  year = {{2025}},\n",
    "  publisher = {{GitHub}},\n",
    "  url = {{https://github.com/codelion/icm}}\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "# Save model card\n",
    "with open(\"./qwen3-reward-model-final/README.md\", \"w\") as f:\n",
    "    f.write(model_card)\n",
    "\n",
    "print(\"‚úÖ Model card created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload-to-hub"
   },
   "outputs": [],
   "source": [
    "# Upload to Hugging Face Hub\n",
    "print(f\"üöÄ Uploading model to Hugging Face Hub: {OUTPUT_MODEL_NAME}\")\n",
    "\n",
    "try:\n",
    "    # Push to hub\n",
    "    model.push_to_hub(\n",
    "        OUTPUT_MODEL_NAME,\n",
    "        token=HF_TOKEN,\n",
    "        private=False,\n",
    "        create_pr=False\n",
    "    )\n",
    "    \n",
    "    tokenizer.push_to_hub(\n",
    "        OUTPUT_MODEL_NAME,\n",
    "        token=HF_TOKEN,\n",
    "        private=False,\n",
    "        create_pr=False\n",
    "    )\n",
    "    \n",
    "    # Upload the README.md file\n",
    "    api = HfApi()\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=\"./qwen3-reward-model-final/README.md\",\n",
    "        path_in_repo=\"README.md\",\n",
    "        repo_id=OUTPUT_MODEL_NAME,\n",
    "        token=HF_TOKEN\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Model successfully uploaded to: https://huggingface.co/{OUTPUT_MODEL_NAME}\")\n",
    "    print(\"üéâ Training and upload completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error uploading model: {str(e)}\")\n",
    "    print(\"üí° You can manually upload the model from './qwen3-reward-model-final/' directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary-section"
   },
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "üéâ **Training Complete!** Your reward model has been successfully trained and uploaded to Hugging Face.\n",
    "\n",
    "### Model Information:\n",
    "- **Model Name:** `codelion/Qwen3-0.6B-icm-rm`\n",
    "- **Base Model:** Qwen/Qwen3-0.6B\n",
    "- **Task:** Binary classification (True/False reward modeling)\n",
    "- **Training Dataset:** ICM-generated dataset\n",
    "\n",
    "### Next Steps:\n",
    "1. **Use for RLHF:** This reward model can now be used to train other language models using Reinforcement Learning from Human Feedback\n",
    "2. **Integration:** Integrate with PPO or other RL frameworks for fine-tuning\n",
    "3. **Evaluation:** Test the model on downstream tasks to evaluate its effectiveness\n",
    "4. **Scaling:** Consider training larger versions or on more diverse datasets\n",
    "\n",
    "### Usage Example:\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load your trained reward model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codelion/Qwen3-0.6B-icm-rm\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"codelion/Qwen3-0.6B-icm-rm\")\n",
    "\n",
    "# Use for reward modeling in RLHF pipelines\n",
    "```\n",
    "\n",
    "The model is now ready for production use! üöÄ"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}